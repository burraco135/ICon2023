<div class="notebook">

<div class="nb-cell markdown" name="md1">
# [ICon2023] Beer Spotter
#### Sistema basato su conoscenza

## Gruppo di lavoro
- Ester Molinari, 716555, e.molinari3@studenti.uniba.it
- Giacomo Signorile, 704897, g.signorile14@studenti.uniba.it

## Introduzione
Questo notebook realizzato in SWISH presenta le funzionalità del KBS realizzato per consigliare le birre.
Prenderemo in esame una porzione minore di fatti rispetto a quelli utilizzati nel progetto finale con il solo scopo di dimostrarne le funzionalità principali.

## Obiettivo
Vogliamo mostrare l'applicazione del Naive Bayes e del KNN con distanza euclidea in Prolog, avendoli implementati anche in Python per confrontarne i risultati.

## Base di conoscenza (KB)
Prendiamo in esame solo i fatti relativi agli stili delle birre. Gli stili totali sono 110, ne selezioneremo solo alcuni.
</div>

<div class="nb-cell program" data-background="true" name="p6">
% id e nome dello stile
styleid(0, altbier).
styleid(1, barleywine).
styleid(10, bock).
styleid(20, ale).
styleid(21, dubbel).
styleid(30, ipa).
styleid(40, lager).
</div>

<div class="nb-cell program" data-background="true" name="p4">
% fatti per l'applicazione del KNN
mouthfeel(0, 2.6, 2.4, 1.7).
mouthfeel(1, 1.9, 2.9, 4.4).
mouthfeel(10, 2.7, 2.4, 2.4).
mouthfeel(10, 1.8, 2.9, 1.9).
mouthfeel(20, 2.4, 3.0, 1.4).
mouthfeel(21, 1.8, 2.4, 2.5).
mouthfeel(30, 2.8, 2.2, 2.4).
mouthfeel(40, 2.9, 1.9, 1.3).

taste(0, 2.8, 2.9, 1.5, 1.1).
taste(1, 3.4, 4.3, 1.9, 1.1).
taste(10, 1.3, 3.5, 1.5, 1.0).
taste(10, 2.2, 3.5, 1.4, 1.1).
taste(20, 1.8, 2.4, 1.6, 1.2).
taste(21, 1.7, 4.3, 1.8, 1.0).
taste(30, 3.6, 2.6, 2.7, 1.1).
taste(40, 2.3, 1.8, 1.3, 1.2).

flavour(0, 1.9, 3.3, 1.4, 3.9).
flavour(1, 3.3, 3.7, 1.6, 3.9).
flavour(10, 2.1, 3.3, 1.4, 3.7).
flavour(10, 1.9, 2.1, 1.5, 3.8).
flavour(20, 1.9, 2.2, 1.4, 2.7).
flavour(21, 3.5, 1.8, 2.0, 3.0).
flavour(30, 4.2, 4.4, 2.1, 1.9).
flavour(40, 1.5, 3.0, 1.2, 2.8).
</div>

<div class="nb-cell program" data-background="true" name="p1">
% fatti per l'applicazione del Naive Bayes
style(altbier, astringent, sweet, malty).
style(barleywine, alcohol, sweet, malty).
style(bock, astringent, sweet, malty).
style(bock, body, sweet, malty).
style(ale, body, sweet, malty).
style(dubbel, alcohol, sweet, fruits).
style(ipa, astringent, bitter, hoppy).
style(ipa, body, bitter, hoppy).
style(lager, astringent, bitter, hoppy).
style(lager, body, sweet, malty).
</div>

<div class="nb-cell markdown" name="md2">
Prendiamo in considerazione i seguenti stili:

- **Altbier**, una birra astringente, dolce e maltosa
- **Barleywine**, una birra alcolica, dolce e maltosa
- **Bock**, una birra astringente o corposa, dolce e maltosa
- **Ale**, una birra corposa, dolce e maltosa
- **Dubbel**, una birra alcolica, dolce e fruttata
- **IPA**, una birra astringente o corposa, amara e luppolata
- **Lager**, una birra astringente o corposa, dolce e maltosa
</div>

<div class="nb-cell markdown" name="md3">
Adesso aggiungiamo una o due birre per ogni stile.
</div>

<div class="nb-cell program" data-background="true" name="p3">
% fatti relativi alle birre per mostrare più informazioni in output
beer(0, "Amber", "Alaskan Brewing Co.", altbier).
beer(1, "Double Bag", "Long Trail Brewing Co.", altbier).
beer(39, "Olde School Barleywine", "Dogfish Head Brewery", barleywine).
beer(40, "Brewer's Reserve Bourbon Barrel Barleywine", "Central Waters Brewing Company", barleywine).
beer(249, "Troegenator", "Troegs Brewing Company", bock).
beer(250, "Spaten Optimator", "Spaten-Franziskaner-Brau", bock).
beer(449, "Over Ale", "Half Acre Beew Company", ale).
beer(450, "Leffe Brune", "Abbaye de Leffe", ale).
beer(584, "Première (red)", "Bières de Chimay", dubbel).
beer(585, "Maudite", "Unibroque", dubbel).
beer(800, "Sculpin", "Ballast Point Brewing Company", ipa).
beer(801, "60 Minute Ipa", "Dogfish head brewery", ipa).
beer(1016, "Budweiser", "Anheuser-Busch", lager).
beer(1017, "Pabst Blue Ribbon (PBR)", "Pabst Brewing Comapny", lager).
</div>

<div class="nb-cell markdown" name="md7">
## "Naive" KNN
Abbiamo voluto chiamare il nostro KNN "Naive" in quanto non applica propriamente le procedure del metodo, invece effettua il calcolo della distanza tra le feature numeriche per tutti i fatti presenti nella KB.
</div>

<div class="nb-cell program" data-background="true" name="p5">
% calcola la distanza euclidea tra i valori dei gruppi di feature
dist_mouthfeel(Astringency, Body, Alcohol) :-
    mouthfeel(A, Ast, Bod, Alc),    
    S is sqrt((Astringency-Ast)^2+(Body-Bod)^2+(Alcohol-Alc)^2),
    
    styleid(A, Name),
    write("Stile (texture): "), write(Name), write(" - Distanza: "), write(S), nl.

dist_taste(Bitter, Sweet, Sour, Salty) :-
    taste(A, Bit, Swe, Sou, Sal),
    S is sqrt((Bitter-Bit)^2+(Sweet-Swe)^2+(Sour-Sou)^2+(Salty-Sal)^2),
    
    styleid(A, Name),
    write("Stile (gusto): "), write(Name), write(" - Distanza: "), write(S), nl.

dist_flavour(Fruity, Hoppy, Spices, Malty) :-
    flavour(A, Fru, Hop, Spi, Mal),
    S is sqrt((Fruity-Fru)^2+(Hoppy-Hop)^2+(Spices-Spi)^2+(Malty-Mal)^2),
    
    styleid(A, Name),
    write("Stile (aroma): "), write(Name), write(" - Distanza: "), write(S), nl.
</div>

<div class="nb-cell markdown" name="md5">
Vogliamo farci consigliare una birra corposa, leggermente astringente e poco alcolica.
Mostriamo il nome dello stile con la sua relativa distanza impostando i valori nel seguente modo:
- **Astringency**: 2
- **Body**: 3
- **Alcohol**: 1
</div>

<div class="nb-cell query" name="q1">
dist_mouthfeel(2, 3, 1).
</div>

<div class="nb-cell markdown" name="md11">
Il valore minimo di distanza lo si ha per lo stile "Ale" (0.56) mentre il valore massimo di distanza lo si ha per lo stile "Barleywine" (3.40). Possiamo quindi affermare che una birra dello stile "Ale" è più corposa che astringente e poco alcolica.

Adesso aggiungiamo che lo stile che vogliamo deve essere leggermente dolce rispetto alle altre feature relative al sapore.
</div>

<div class="nb-cell query" name="q4">
dist_taste(1, 2, 1, 1).
</div>

<div class="nb-cell markdown" name="md12">
Il valore minimo di distanza lo si ha per lo stile "Ale" (1.09) mentre il valore massimo di distanza lo si ha per lo stile "Barleywine" (3.44). Possiamo quindi affermare che una birra dello stile "Ale" è più dolce rispetto alle altre.

Continuiamo la ricerca dello stile aggiungendo che vogliamo un aroma fruttato.
</div>

<div class="nb-cell query" name="q5">
dist_flavour(2, 1, 1, 1).
</div>

<div class="nb-cell markdown" name="md13">
Il **valore minimo** di distanza lo si ha per lo stile "Ale" (2.12) mentre il **valore massimo** di distanza lo si ha per lo stile "IPA" (4.29). Possiamo quindi affermare che una birra dello stile "Ale" è più fruttata rispetto alle altre.
</div>

<div class="nb-cell markdown" name="md4">
Analizzando i dati, possiamo quindi consigliare una birra dello **stile "Ale"** per un utente che cerca una birra **corposa, dolce e fruttata**.

Effettuando una ricerca riguardante lo stile "Ale" e "Barleywine" scopriamo che:
- **Ale** è il termine usato per indicare le birre ad alta fermentazione, [...]. Il risultato è un sapore dolce, dal corpo pieno e fruttato.
- Il **barley wine** o **barleywine** [...] è un tipo di birra ad alta fermentazione di origine britannica. Ha questo nome perché è la birra che si accosta di più al vino sia per i sapori che per il tasso alcolico che risulta elevato rispetto alla media delle birre.
</div>

<div class="nb-cell markdown" name="md6">
## Naive Bayes
Adesso realizziamo un classificatore **Naive Bayes** per consigliare uno stile di birra basato su feature categoriche e non numeriche. Abbiamo voluto applicare uno **smoothing di Laplace** con **K = 0.1** alla probabilità a priori dello stile e alla probabilità condizionata delle feature categoriche.

Abbiamo scelto un valore molto basso per evitare che si verifichi una sovrastima delle probabilità, quindi abbiamo preferito fare un modello incline a **sottostimare le probabilità**.
</div>

<div class="nb-cell program" data-background="true" name="p2">
% calcolo la probabilità a priori dello stile
count_style(Name, Count) :-
    findall(Name, style(Name, _, _, _), L),
    length(L, Count).

prob_style(Name, Prob) :-
    count_style(Name, N),
    count_style(_, T),
    C is T+0.2,
    Prob is (N+0.1)/C.

% calcolo la probabilità condizionata della prima feature
count_mouthfeel(Mouthfeel, Name, Count) :-
    findall(Mouthfeel, style(Name, Mouthfeel, _, _), L),
    length(L, Count).

prob_mouthfeel(Mouthfeel, Name, Prob) :-
    count_mouthfeel(Mouthfeel, Name, N),
    count_style(Name, T),
    C is T+0.3,
    Prob is (N+0.1)/C.

% calcolo la probabilità condizionata della seconda feature
count_taste(Taste, Name, Count) :-
    findall(Taste, style(Name, _, Taste, _), L),
    length(L, Count).

prob_taste(Taste, Name, Prob) :-
    count_taste(Taste, Name, N),
    count_style(Name, T),
    C is T+0.3,
    Prob is (N+0.1)/C.

% calcolo la probabilità condizionata della terza feature
count_flavour(Flavour, Name, Count) :-
    findall(Flavour, style(Name, _, _, Flavour), L),
    length(L, Count).

prob_flavour(Flavour, Name, Prob) :-
    count_flavour(Flavour, Name, N),
    count_style(Name, T),
    C is T+0.3,
    Prob is (N+0.1)/C.

% calcolo della formula Naive Bayes
predict(Name, Mouthfeel, Taste, Flavour, Prob) :-
    prob_style(Name, P1),
    prob_mouthfeel(Mouthfeel, Name, P2),
    prob_taste(Taste, Name, P3),
    prob_flavour(Flavour, Name, P4),
    Prob is P1*P2*P3*P4.
</div>

<div class="nb-cell markdown" name="md8">
Facciamo calcolare la probabilità che uno stile sia **alcolico** (A = alcohol), **dolce** (B = sweet) e **fruttato** (C = fruits).
</div>

<div class="nb-cell query" name="q2">
A = body, B = sweet, C = fruits,

predict(altbier, A, B, C, Altbier),
predict(barleywine, A, B, C, Barleywine),
predict(bock, A, B, C, Bock),
predict(ale, A, B, C, Ale),
predict(dubbel, A, B, C, Dubbel),
predict(ipa, A, B, C, IPA),
predict(lager, A, B, C, Lager).
</div>

<div class="nb-cell markdown" name="md9">
A pari merito, troviamo gli stili "Ale" e "Dubbel" con una probabilità di consigliare lo stile dello **0.59%** rispetto a tutte le altre.

Proviamo questa volta a mettere i dati di uno stile di cui **non si conosce la classificazione** per mancanza di dati di addestramento (ad esempio, "sour" non è una feature presente nella conoscenza di base).
</div>

<div class="nb-cell query" name="q3">
A = alcohol, B = sour, C = malty,

predict(altbier, A, B, C, Altbier),
predict(barleywine, A, B, C, Barleywine),
predict(bock, A, B, C, Bock),
predict(ale, A, B, C, Ale),
predict(dubbel, A, B, C, Dubbel),
predict(ipa, A, B, C, IPA),
predict(lager, A, B, C, Lager).
</div>

<div class="nb-cell markdown" name="md10">
Il valore di probabilità più alto si ha con lo stile "Barleywine" (0.59%), quindi una birra con queste caratteristiche è più probabile che appartenga allo stile "Barleywine" che allo stile "IPA", che registra la percentuale più bassa.
</div>

<div class="nb-cell markdown" name="md14">
## Consigliatore Prolog
Eseguiamo le ultime due query come query semplici in Prolog per confrontare la soluzione con quelle precedenti.
</div>

<div class="nb-cell query" name="q6">
style(Style1, body, sweet, fruits),
style(Style2, alcohol, sour, malty).
</div>

<div class="nb-cell markdown" name="md15">
Inserendo i dati delle query precedenti otteniamo "false" in quanto nella base di conoscenza **non esiste** alcuno stile di birra che è corposo, dolce e fruttato e non esiste uno stile alcolico, aspro e maltato.
</div>

<div class="nb-cell markdown" name="md17">
### Precisione e richiamo
Vogliamo ottenere i valori di precisione e richiamo del Naive Bayes indicato sopra.
Prendiamo come dati di test:
- **Altbier**: (0, 2.6, 2.4, 1.7), (0, 2.8, 2.9, 1.5, 1.1), (0, 1.9, 3.3, 1.4, 3.9)
- **Lager**: (40, 2.9, 1.9, 1.3), (40, 2.3, 1.8, 1.3, 1.2), (40, 1.5, 3.0, 1.2, 2.8)
- **Bock**: (10, 2.7, 2.4, 2.4), (10, 1.3, 3.5, 1.5, 1.0), (10, 2.1, 3.3, 1.4, 3.7)
- **IPA**: (30, 2.8, 2.2, 2.4), (30, 3.6, 2.6, 2.7, 1.1), (30, 4.2, 4.4, 2.1, 1.9)
---
- **Altbier**: astringent, sweet, malty
- **Lager**: astringent, bitter, hoppy
- **Bock**: body, sweet, malty
- **IPA**: body, bitter, hoppy
</div>

<div class="nb-cell markdown" name="md20">
#### Naive Bayes
</div>

<div class="nb-cell query" name="q7">
% altbier (mouthfeel)
dist_mouthfeel(3, 2, 2).
</div>

<div class="nb-cell markdown" name="md24">
Il valore minore di distanza è 0.48 e corrisponde a IPA, ma noi avevamo inserito i valori di una Altbier.
</div>

<div class="nb-cell query" name="q8">
% lager (mouthfeel)
dist_mouthfeel(3, 2, 1)
</div>

<div class="nb-cell markdown" name="md25">
Il valore minore di distanza è 0.33 e corrisponde a Lager; noi avevamo inserito i valori di una Lager per difetto ed eccesso.
</div>

<div class="nb-cell query" name="q9">
% bock (mouthfeel)
dist_mouthfeel(3, 2, 2)
</div>

<div class="nb-cell markdown" name="md26">
Il valore minore di distanza è 0.48 e corrisponde a IPA, ma noi avevamo inserito i valori di una Bock per difetto e per eccesso.
</div>

<div class="nb-cell query" name="q10">
% IPA (mouthfeel)
dist_mouthfeel(3, 2, 2)
</div>

<div class="nb-cell markdown" name="md27">
Il valore minore di distanza è 0.48 e corrisponde a IPA; noi avevamo inserito i valori di una IPA arrotondati per difetto e per eccesso.
</div>

<div class="nb-cell query" name="q11">
% altbier (taste)
dist_taste(3, 3, 2, 1)
</div>

<div class="nb-cell markdown" name="md28">
Il valore minore di distanza è 0.55 e corrisponde a Altbier; noi avevamo inserito i valori di una Altbier arrotondati per difetto e per eccesso.
</div>

<div class="nb-cell query" name="q12">
% lager (taste)
dist_taste(2, 2, 1, 1)
</div>

<div class="nb-cell markdown" name="md29">
Il valore minore di distanza è 0.50 e corrisponde a Lager; noi avevamo inserito i valori di una Lager arrotondati per difetto e per eccesso.
</div>

<div class="nb-cell query" name="q13">
% bock (taste)
dist_taste(1, 4, 2, 1)
</div>

<div class="nb-cell markdown" name="md30">
Il valore minore di distanza è 0.76 e corrisponde a Bock; noi avevamo inserito i valori di una Bock arrotondati per difetto e per eccesso.
</div>

<div class="nb-cell query" name="q14">
% IPA (taste)
dist_taste(4, 3, 3, 1)
</div>

<div class="nb-cell markdown" name="md31">
Il valore minore di distanza è 0.64 e corrisponde a IPA; noi avevamo inserito i valori di una IPA arrotondati per difetto e per eccesso.
</div>

<div class="nb-cell query" name="q15">
% altbier (flavour)
dist_flavour(2, 3, 1, 4)
</div>

<div class="nb-cell markdown" name="md32">
Il valore minore di distanza è 0.51 e corrisponde a Altbier; noi avevamo inserito i valori di una Altbier arrotondati per difetto e per eccesso.
</div>

<div class="nb-cell query" name="q16">
% lager (flavour)
dist_flavour(2, 3, 1, 3)
</div>

<div class="nb-cell markdown" name="md33">
Il valore minore di distanza è 0.86 e corrisponde a Lager; noi avevamo inserito i valori di una Lager arrotondati per difetto e per eccesso.
</div>

<div class="nb-cell query" name="q17">
% bock (flavour)
dist_flavour(2, 3, 1, 4)
</div>

<div class="nb-cell markdown" name="md34">
Il valore minore di distanza è 0.51 e corrisponde a Altbier, ma noi avevamo inserito i valori di una Bock arrotondati per difetto e per eccesso.
</div>

<div class="nb-cell query" name="q18">
% IPA (flavour)
dist_flavour(4, 4, 2, 2)
</div>

<div class="nb-cell markdown" name="md35">
Il valore minore di distanza è 0.46 e corrisponde a IPA; noi avevamo inserito i valori di una IPA arrotondati per difetto e per eccesso.
</div>

<div class="nb-cell markdown" name="md18">
Effettuiamo il calcolo della precisione e del richiamo per ogni gruppo di feature:
- Mouthfeel
	- **Precisione**: 2/4 = 0.5
    - **Richiamo**: 2/2 = 1.0
- Taste
	- **Precisione**: 4/4 = 1.0
    - **Richiamo**: 4/4 = 1.0
- Flavour
	- **Precisione**: 3/4 = 0.75
    - **Richiamo**: 3/3 = 1.0
</div>

<div class="nb-cell markdown" name="md19">
Possiamo dire che il nostro Naive Bayes ha una precisione media di 0.75 e un richiamo di 1.0 su dei dati presenti nella conoscenza di base.
</div>

<div class="nb-cell markdown" name="md21">
#### "Naive" KNN
</div>

<div class="nb-cell query" name="q19">
% altbier
A = astringent, B = sweet, C = malty,

predict(altbier, A, B, C, Altbier),
predict(barleywine, A, B, C, Barleywine),
predict(bock, A, B, C, Bock),
predict(ale, A, B, C, Ale),
predict(dubbel, A, B, C, Dubbel),
predict(ipa, A, B, C, IPA),
predict(lager, A, B, C, Lager).
</div>

<div class="nb-cell query" name="q20">
% lager
A = astringent, B = bitter, C = malty,

predict(altbier, A, B, C, Altbier),
predict(barleywine, A, B, C, Barleywine),
predict(bock, A, B, C, Bock),
predict(ale, A, B, C, Ale),
predict(dubbel, A, B, C, Dubbel),
predict(ipa, A, B, C, IPA),
predict(lager, A, B, C, Lager).
</div>

<div class="nb-cell query" name="q21">
% bock
A = body, B = sweet, C = malty,

predict(altbier, A, B, C, Altbier),
predict(barleywine, A, B, C, Barleywine),
predict(bock, A, B, C, Bock),
predict(ale, A, B, C, Ale),
predict(dubbel, A, B, C, Dubbel),
predict(ipa, A, B, C, IPA),
predict(lager, A, B, C, Lager).
</div>

<div class="nb-cell query" name="q22">
%IPA
A = body, B = bitter, C = hoppy,

predict(altbier, A, B, C, Altbier),
predict(barleywine, A, B, C, Barleywine),
predict(bock, A, B, C, Bock),
predict(ale, A, B, C, Ale),
predict(dubbel, A, B, C, Dubbel),
predict(ipa, A, B, C, IPA),
predict(lager, A, B, C, Lager).
</div>

<div class="nb-cell markdown" name="md22">
Effettuiamo il calcolo della precisione e del richiamo per tutte le query:
- **Precisione**: 3/4 = 0.75
- **Richiamo**: 3/3 = 1.0
</div>

<div class="nb-cell markdown" name="md23">
### Prolog
Non è necessario calcolare la precisione e il richiamo del consigliatore Prolog in quanto il sistema è CWA (Closed World Assumption), quindi se uno stile non esiste, restituirà "false" ottenendo una precisione pari a 1.0 e un richiamo pari a 1.0.
</div>

<div class="nb-cell markdown" name="md16">
## Conclusioni
### "Naive" KNN
Questo classificatore fornisce un risultato accettabile, basandosi unicamente sulle feature numeriche, quindi non richiede l'utilità di un esperto che analizzi i dati e li converta in feature categoriche.

### Naive Bayes
Il Naive Bayes fornisce percentuali molto basse ma abbastanza accurate, e il suo risultato è simile a quello del "Naive" KNN. L'unico problema è che richiede l'analisi delle feature numeriche e la loro conversione in feature categoriche da parte di un esperto.

### Prolog
Il risultato del consigliatore Prolog non è adatto alla classificazione degli stili di birra. Nel caso in cui non dovesse esistere una feature categorica, il sistema non è in grado di consigliare nessuno stile.
</div>

</div>
